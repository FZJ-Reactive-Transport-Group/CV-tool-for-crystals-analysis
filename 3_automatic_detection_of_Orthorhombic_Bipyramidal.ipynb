{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ce7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block imports essential libraries. No changes are required here \n",
    "# for new users unless additional functionality is needed.\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import trimesh\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel sheet into a DataFrame\n",
    "\n",
    "# Specifying the path to the Excel file. This file is expected to be created by a separate script named '1_cropping_desired_crystal_coordinates.ipynb'.\n",
    "excel_path = 'D:/Radium/Results_auto/Input_Parameters.xlsx' # make sure that the input folder is for Orthorhombic Bipyramidal crstals\n",
    "\n",
    "# Loading a 3D mesh from an STL file. This STL file represents a Orthorhombic Bipyramidal crystal structure.\n",
    "your_mesh = trimesh.load_mesh('D:/Radium/Raman_3D/Crystal_type_5/crystal5.stl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the Molar Volume value to be used later in the calculation later in the script\n",
    "# Molar volume (cm^3 mol^-1)\n",
    "molar_volume_cm3 = 53.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d01e6",
   "metadata": {},
   "source": [
    "This code block defines two functions for specific tasks:\n",
    "\n",
    "    Finding Input Folders: The find_xy_folders function is designed to locate folders within a specified root directory that match a naming pattern based on an 'XY number'. It uses the glob module to search through directories recursively. This function is useful for organizing or categorizing files and folders based on naming conventions. This function is compatible with the current processing method that destiguishes different captured locations in the microfluidic chip and identify each region with a distinct (XY) number. If this process method is changed, then this function should be changed accordingly.\n",
    "\n",
    "    Calculating Tetrahedron Volume: The tetrahedron_volume function calculates the volume of a tetrahedron (a four-sided 3D shape) given the coordinates of its four vertices. This calculation is performed using vector operations (cross and dot products) provided by NumPy, illustrating the function's application in geometrical or 3D mesh analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find input folders\n",
    "\n",
    "# Define a function that finds folders based on a given XY number pattern within a specified root path.\n",
    "def find_xy_folders(root_path, xy_number):\n",
    "    # Forming a search pattern using the root_path and xy_number. The '**' allows for searching in all subdirectories.\n",
    "    search_pattern = os.path.join(root_path, '**', f'XY{xy_number}')\n",
    "    # Uncomment the following line to print the search pattern for debugging.\n",
    "    # print(search_pattern)\n",
    "\n",
    "    # Using glob to find all folders that match the search pattern. The 'recursive=True' parameter allows searching in all subdirectories.\n",
    "    input_folders = glob(search_pattern, recursive=True)\n",
    "    # Uncomment the following line to print the list of found folders for debugging.\n",
    "    # print(input_folders)\n",
    "\n",
    "    # Returning the list of folders found.\n",
    "    return input_folders\n",
    "\n",
    "# Function to calculate the volume of a tetrahedron\n",
    "\n",
    "# Define a function to calculate the volume of a tetrahedron given its four vertices (a, b, c, d).\n",
    "def tetrahedron_volume(a, b, c, d):\n",
    "    # Calculating the volume using a formula based on the cross product and dot product.\n",
    "    return np.abs(np.dot(a-d, np.cross(b-d, c-d))) / 6.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60de6a0",
   "metadata": {},
   "source": [
    "This code block performs complex image processing to identify and analyze rhomboid shapes in images. It integrates these results with 3D mesh volume calculations and consolidates data from various sources into cohesive DataFrames. The results are saved and summarized, providing a comprehensive output of the analysis.\n",
    "Usage Tips:\n",
    "\n",
    "   - Ensure that all prerequisite files (images, CSV, Excel) are correctly placed in the specified directories.\n",
    "   - Adjust parameters (like contour area thresholds, scaling factors) according to the specifics of your dataset and analysis requirements.\n",
    "   - Review the outputs (saved images, CSV files) regularly to ensure that the processing is happening as expected.\n",
    "\n",
    "Note on Adjusting Parameters:\n",
    "\n",
    "   - Bilateral Filter: The parameters 9, 75, 75 in cv2.bilateralFilter can be adjusted. The first one is the diameter of the pixel neighborhood, the second is the filter sigma in the color space, and the third is the filter sigma in the coordinate space. Adjust these for different levels of noise reduction and edge preservation.\n",
    "   - Adaptive Thresholding: The 15 and 3 in cv2.adaptiveThreshold are the block size and a constant subtracted from the mean, respectively. Vary these to change the binarization effect based on local image characteristics.\n",
    "\n",
    "Explanation of Parameters and Processes:\n",
    "\n",
    "   - Contour Finding: The cv2.findContours function detects contours in an image. cv2.RETR_EXTERNAL retrieves only the external contours, and cv2.CHAIN_APPROX_NONE stores the full contour without compression.\n",
    "\n",
    "   - Contour Area: cv2.contourArea calculates the area of each contour. Contours smaller than min_contour_area (50 pixels in this case) are ignored.\n",
    "\n",
    "   - Nested Contours: The code checks for contours nested within other contours to filter them out. This is done by comparing the bounding rectangles of each contour pair.\n",
    "\n",
    "   - Rhomboid Detection: The script approximates each contour to a polygon and checks if it has four sides and angles within a specified threshold (angle_threshold), which is set to 25 degrees. Adjust this value to be more or less strict about the shape's angular accuracy.\n",
    "   - Skipping Images: If skip_image is True, the original image is saved without further processing, and a row with empty analysis results is added to the DataFrame.\n",
    "\n",
    "   - Rhomboid Contour Processing: If multiple rhomboid contours are found, the code retains only the largest one based on the contour area.\n",
    "\n",
    "   - Drawing and Analyzing Rhomboids: The script draws contours around the detected rhomboids and calculates their diagonal lengths and area. The area is then converted from pixels to micrometers squared using the conversion_rate.\n",
    "   - 3D Mesh Volume Calculation: The code calculates the volume of a 3D mesh by summing the volumes of tetrahedra formed by the mesh faces and the origin. It then scales the mesh based on the rhomboid dimensions and recalculates the volume and surface area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a71940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the Excel sheet into a DataFrame\n",
    "df = pd.read_excel(excel_path, dtype={'xy_number': str})  # Read xy_number as string\n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "# New DataFrame for crystal size\n",
    "crystal_size_df = pd.DataFrame(columns=['exp_number', 'folder_number', 'diagonal_1'])\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Initialize a variable to store the previous area value\n",
    "    previous_area = 0.0  \n",
    "    # Extracting values from the row\n",
    "    exp_number = row['exp_number']\n",
    "    root_path = row['root_path']\n",
    "    xy_number = row['xy_number']  # This will be a string\n",
    "    folder_name = row['folder_name']\n",
    "    folder_number = int(re.search(r'\\d+', folder_name).group())\n",
    "    coord_str = row['coord_str']\n",
    "    \n",
    "    # Use your function to find input folders\n",
    "    input_folders = find_xy_folders(root_path, xy_number)\n",
    "    \n",
    "#     # Extract coordinates from coord_str\n",
    "    match = re.search(r\"\\[(\\d+):(\\d+), (\\d+):(\\d+)\\]\", coord_str)\n",
    "    if match:\n",
    "        x_start, x_end, y_start, y_end = map(int, match.groups())\n",
    "\n",
    "    found_crystals = 0\n",
    "\n",
    "    # Construct the output folder path\n",
    "    output_folder = f\"D:/Radium/Results_auto/{exp_number}/{folder_name}/\"\n",
    "    conversion_rate = 0.18  # micrometers/pixel\n",
    "    output_csv_filename = f\"{exp_number}_Orthorhombic_Bipyramidal.csv\"\n",
    "    \n",
    "    # Creating the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Loop through each folder in the list\n",
    "    image_paths = []\n",
    "    for input_folder in input_folders:\n",
    "        image_paths.extend(glob(os.path.join(input_folder, \"*.tif\")))\n",
    "        \n",
    "    # Creating a DataFrame for storing image analysis results\n",
    "    df = pd.DataFrame(columns=[\"ImageName\", \"RectangularArea_micrometer^2\", \n",
    "                               \"Volume_micrometer^3\", \"SurfaceArea_micrometer^2\",\n",
    "                               \"Number_of_Moles\"])\n",
    "    \n",
    "    # Looping through each image path\n",
    "    for image_path in image_paths:\n",
    "        # Flag to determine if the image should be skipped\n",
    "        skip_image = False  # Initialize the flag\n",
    "        # Load the image in grayscale mode using OpenCV\n",
    "        img = cv2.imread(image_path, 0)\n",
    "        # Convert the grayscale image to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Crop the image using the coordinates extracted from 'coord_str'\n",
    "        img = img[x_start:x_end, y_start:y_end]\n",
    "        \n",
    "        # Apply bilateral filter to the image for noise reduction while keeping edges sharp\n",
    "        # Parameters: source image, diameter of pixel neighborhood (9), \n",
    "        # color space filter sigma (75), coordinate space filter sigma (75)\n",
    "        img_DN = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "        gray = cv2.cvtColor(img_DN, cv2.COLOR_BGR2GRAY) # Convert the filtered image to grayscale\n",
    "        \n",
    "        # Apply adaptive thresholding to convert the grayscale image to a binary image\n",
    "        # Parameters: source image, max value (255), adaptive method (Gaussian),\n",
    "        # threshold type (binary), block size (15), constant subtracted from mean (3)        \n",
    "        thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                       cv2.THRESH_BINARY, 15, 3)\n",
    "        # Define a sharpening kernel\n",
    "        kernel = np.array([[-1, -1, -1],\n",
    "                           [-1,  9, -1],\n",
    "                           [-1, -1, -1]])\n",
    "        # Apply the kernel to the thresholded image to sharpen it\n",
    "        sharpened = cv2.filter2D(thresh, -1, kernel)\n",
    "        # Apply the Laplacian operator to highlight regions of rapid intensity change\n",
    "        laplacian = cv2.Laplacian(sharpened, cv2.CV_64F)\n",
    "        # Convert the Laplacian result to an 8-bit absolute value to ensure proper image format\n",
    "        laplacian_abs = cv2.convertScaleAbs(laplacian)\n",
    "        \n",
    "        # Find contours in the laplacian_abs image using external contours only\n",
    "        # and storing each contour's points as-is (without any compression)        \n",
    "        contours, _ = cv2.findContours(laplacian_abs, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        \n",
    "        # Define a minimum contour area to filter out small contours\n",
    "        min_contour_area = 50\n",
    "        valid_contours = []\n",
    "        \n",
    "        # Iterate through each contour found\n",
    "        for contour in contours:\n",
    "            # Calculate the area of the contour\n",
    "            contour_area = cv2.contourArea(contour)\n",
    "            # Add contour to valid_contours list if its area is above the threshold\n",
    "            if contour_area >= min_contour_area:\n",
    "                valid_contours.append(contour)\n",
    "        # If no valid contours are found, set the skip_image flag to True\n",
    "        if not valid_contours:  # Check if the list is empty\n",
    "            skip_image = True\n",
    "            \n",
    "        # Initialize a list to store filtered contours after nested contour check\n",
    "        filtered_contours = []\n",
    "        \n",
    "        # Iterate through valid_contours to filter out nested contours\n",
    "        for i, contour in enumerate(valid_contours):\n",
    "            is_nested = False\n",
    "            # Get the bounding rectangle of the contour\n",
    "            (x, y, w, h) = cv2.boundingRect(contour)\n",
    "            # Check if this contour is nested within another\n",
    "            for j, inner_contour in enumerate(valid_contours):\n",
    "                if i != j:\n",
    "                    (x_inner, y_inner, w_inner, h_inner) = cv2.boundingRect(inner_contour)\n",
    "                    # If the inner contour is completely within the outer contour, mark as nested        \n",
    "                    if x_inner >= x and y_inner >= y and x_inner + w_inner <= x + w and y_inner + h_inner <= y + h:\n",
    "                        is_nested = True\n",
    "                        break\n",
    "            # If contour is not nested, add it to filtered_contours list\n",
    "            if not is_nested:\n",
    "                filtered_contours.append(contour)\n",
    "        # If no filtered_contours are found and skip_image is not already set to True, set it to True\n",
    "        if not filtered_contours and not skip_image:  # Check if the list is empty and skip_image is still False\n",
    "            skip_image = True\n",
    "            \n",
    "        # Initialize a list to store contours that are identified as rhomboid shapes\n",
    "        rhomboid_contours = []\n",
    "\n",
    "        # Process each contour in filtered_contours to identify rhomboids\n",
    "        for contour in filtered_contours:\n",
    "            # Approximate contour to a polygon and calculate its perimeter\n",
    "            epsilon = 0.05 * cv2.arcLength(contour, True)\n",
    "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "            \n",
    "            # Check if the approximated polygon has 4 sides (potential rhomboid)\n",
    "            if len(approx) == 4:\n",
    "                # Calculate lengths between points in the contour\n",
    "                lengths = []\n",
    "                for i in range(4):\n",
    "                    p1 = approx[i][0]\n",
    "                    p2 = approx[(i + 1) % 4][0]\n",
    "                    length = np.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)\n",
    "                    lengths.append(length)\n",
    "\n",
    "                # Sort the lengths\n",
    "                sorted_lengths = sorted(lengths)\n",
    "\n",
    "                # Check if two sides are at least 30% longer than the other two\n",
    "                if sorted_lengths[2] >= 1.3 * sorted_lengths[1] and sorted_lengths[3] >= 1.3 * sorted_lengths[0]:\n",
    "                    rectangular_contours.append(contour)\n",
    "        if not rectangular_contours and not skip_image:  # Check if the list is empty and skip_image is still False\n",
    "            skip_image = True            \n",
    "        # If any of the lists were empty, skip the current image_path\n",
    "        if skip_image:\n",
    "            # Save the original (cropped) image to the output path            \n",
    "            output_filename = os.path.basename(image_path)\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "            cv2.imwrite(output_path, img)\n",
    "            # Add a new row to the DataFrame with empty values for analysis results\n",
    "            new_row = pd.DataFrame({\n",
    "            \"ImageName\": [os.path.basename(image_path)],\n",
    "            \"RectangularArea_micrometer^2\": [\"\"],\n",
    "            \"Volume_micrometer^3\": [\"\"],\n",
    "            \"SurfaceArea_micrometer^2\": [\"\"],\n",
    "            \"Number_of_Moles\" : [\"\"]\n",
    "            })\n",
    "            # Concatenate the new row to the DataFrame\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "            # Continue to the next image in the loop\n",
    "            continue\n",
    "            \n",
    "        # Increment the found_crystals counter if valid rhomboid contours are found\n",
    "        found_crystals += 1\n",
    "        # If more than one rhomboid contour is found, sort them by area and keep only the largest one\n",
    "        if len(rectangular_contours) > 1:\n",
    "            # Sort the contours by area in descending order\n",
    "            rectangular_contours.sort(key=cv2.contourArea, reverse=True)\n",
    "            # Keep only the largest contour\n",
    "            rectangular_contours = [rectangular_contours[0]]\n",
    "            \n",
    "        # Make a copy of the cropped image for drawing contours\n",
    "        rectangular_image = img.copy()\n",
    "        # Draw green contours around the identified rhomboids\n",
    "        cv2.drawContours(rhomboid_image, rectangular_contours, -1, (0, 255, 0), thickness=1)\n",
    "\n",
    "        # Process each rhomboid contour for detailed analysis        \n",
    "        for contour in rhomboid_contours:\n",
    "            # Get the rotated bounding rectangle\n",
    "            rect = cv2.minAreaRect(contour)\n",
    "            box = cv2.boxPoints(rect)\n",
    "            box = np.int0(box)\n",
    "            rhomboid_color = (255, 0, 0)  # Blue color\n",
    "            # Draw the rectangle\n",
    "            cv2.drawContours(rectangular_image, [rhomboid_vertices], -1, rhomboid_color, thickness=1)\n",
    "            \n",
    "            # Calculate the area of the rectangle\n",
    "            side1 = np.linalg.norm(box[0] - box[1])\n",
    "            side2 = np.linalg.norm(box[1] - box[2])\n",
    "            area = side1 * side2\n",
    "            # Convert the area from pixels to micrometers\n",
    "            area_micrometer = area * (conversion_rate ** 2)\n",
    "\n",
    "        # Second part: 3D mesh volume calculation\n",
    "        volume = 0.0\n",
    "        # Iterate over the faces of the mesh and sum up the volumes of the tetrahedra formed with the origin\n",
    "        for face in your_mesh.triangles:\n",
    "            volume += tetrahedron_volume(face[0], face[1], face[2], np.array([0., 0., 0.]))\n",
    "        \n",
    "        # Calculate scaling factors for the mesh based on the rhomboid dimensions\n",
    "        scale_x, scale_z = diagonal1 * conversion_rate, diagonal2 * conversion_rate\n",
    "        # This is based on the relationship between the 210 and the 001 faces.\n",
    "        # This number is not necessary correct for the Orthorhombic Bipyramidal crystals and therefore must be checked first\n",
    "        scale_y = scale_x/4 \n",
    "        # Limit the scale_y to a maximum value\n",
    "        if scale_y > 10:\n",
    "            scale_y = 10\n",
    "        \n",
    "        # Obtain the original dimensions of the mesh\n",
    "        original_dims = your_mesh.bounds[1] - your_mesh.bounds[0]\n",
    "        original_x, original_y, original_z = original_dims\n",
    "\n",
    "        # Calculate the new scaling factors for each dimension\n",
    "        new_scale_x = scale_x / original_x\n",
    "        new_scale_y = scale_y / original_y\n",
    "        new_scale_z = scale_z / original_z\n",
    "\n",
    "        # Apply the new scaling factors to the mesh\n",
    "        scaled_mesh = your_mesh.apply_scale([new_scale_x, new_scale_y, new_scale_z])\n",
    "\n",
    "        # Recalculate the volume of the scaled mesh\n",
    "        scaled_volume = 0.0 # initialize the volume size parameter\n",
    "        for face in scaled_mesh.triangles:\n",
    "            scaled_volume += tetrahedron_volume(face[0], face[1], face[2], np.array([0., 0., 0.]))\n",
    "\n",
    "        # Calculate surface area of the scaled mesh\n",
    "        surface_area = np.sum(scaled_mesh.area_faces)\n",
    "        # Calculate surface area in m^2\n",
    "        surface_area_m2 = surface_area * 1e-12  # Convert from um^2 to m^2\n",
    "        # Convert volume from micrometer^3 to cm^3\n",
    "        scaled_volume_cm3 = scaled_volume / 1e12  # 1 cm^3 = 10^12 micrometer^3\n",
    "        # Calculate the number of moles\n",
    "        number_of_moles = scaled_volume_cm3 / molar_volume_cm3\n",
    "        \n",
    "        # Add the calculated data to the DataFrame\n",
    "        new_row = pd.DataFrame({\n",
    "            \"ImageName\": [os.path.basename(image_path)],\n",
    "            \"RectangularArea_micrometer^2\": [area_micrometer],\n",
    "            \"Volume_micrometer^3\": [scaled_volume],\n",
    "            \"SurfaceArea_micrometer^2\": [surface_area],\n",
    "            \"Number_of_Moles\": [number_of_moles]\n",
    "        })\n",
    "\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        \n",
    "        # Save the processed image with drawn contours\n",
    "        output_filename = os.path.basename(image_path)\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        cv2.imwrite(output_path, rhomboid_image)\n",
    "\n",
    "        # add diagonal length to crystal_size database to calculate the crystal size\n",
    "        new_row = pd.DataFrame({\n",
    "            'exp_number': [exp_number],\n",
    "            'folder_number': folder_number,\n",
    "            'diagonal_1': [diagonal1 * conversion_rate],  # Convert to micrometers\n",
    "        })\n",
    "        crystal_size_df = pd.concat([crystal_size_df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    output_csv_path = os.path.join(output_folder, output_csv_filename)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    output_csv_path = os.path.join(output_folder, output_csv_filename)\n",
    "    # Read the CSV file that was just saved\n",
    "    csv_df = pd.read_csv(output_csv_path)\n",
    "    # Read an additional Excel file for comparison\n",
    "    excel_df = pd.read_excel(f'D:/Radium/Results/time_{exp_number}.xlsx')\n",
    "    # Print the number of rows in both DataFrames for comparison\n",
    "    print(csv_df.shape[0])\n",
    "    print(excel_df.shape[0])\n",
    "    \n",
    "    # Check if the number of rows in both DataFrames is the same\n",
    "    if csv_df.shape[0] != excel_df.shape[0]:\n",
    "        print(\"Warning: The number of columns in the CSV and Excel files are not the same.\")\n",
    "    # Store the number of rows in each DataFrame\n",
    "    csv_rows = csv_df.shape[0]\n",
    "    excel_rows = excel_df.shape[0]\n",
    "    if csv_rows != excel_rows:\n",
    "        print(\"Warning: The number of rows in the CSV and Excel files are not the same.\")\n",
    "        print(csv_df.shape[0])\n",
    "        print(excel_df.shape[0])\n",
    "        # Trim the Excel DataFrame if it has more rows than the CSV DataFrame\n",
    "        if excel_rows > csv_rows:\n",
    "            excel_df = excel_df.iloc[:csv_rows]\n",
    "    # Join the columns from the Excel DataFrame to the CSV DataFrame\n",
    "    joined_df = pd.concat([csv_df, excel_df], axis=1)\n",
    "\n",
    "    # Save the joined DataFrame to a new CSV file\n",
    "    joined_df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    # Print summary information about the processing\n",
    "    print(found_crystals, exp_number, folder_name)\n",
    "    print(\"-----------------\")\n",
    "    \n",
    "    # Add 'exp_number' and 'folder_name' to the DataFrame\n",
    "    df['exp_number'] = exp_number\n",
    "    df['folder_name'] = folder_name\n",
    "    df['time_seconds'] = excel_df['time_min']*60\n",
    "\n",
    "    # Concatenate this DataFrame with the main DataFrame\n",
    "    main_df = pd.concat([main_df, df], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d28ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the main DataFrame to a single CSV file\n",
    "main_df.to_csv(\"D:/Radium/Results_auto/OB_results/All_Experiments_OB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47699f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
